{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOJ55GkndfDFhlLRqwMrFqD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rashwinr/MONAI_tutorials/blob/main/MONAI_Tutorial_DS261_AIMIA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Coding Tutorials on MONAI 4 AI in Medical Image Analysis**\n",
        "\n",
        "This tutorial was conducted as a part of DS261 3:1 Artificial Intelligence for Medical Image Analysis course offered in August of 2024 at the [Computational Data Sciences Department](https://cds.iisc.ac.in/), Indian Institute of Science Bengaluru."
      ],
      "metadata": {
        "id": "FkAq6T2Z66y3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MONAI**\n",
        "<center><img src=\"https://github.com/Project-MONAI/monai-bootcamp/blob/main/MONAICore/monai.png?raw=1\"/></center>\n",
        "\n",
        "\"MONAI\" stands for **Medical Open Network for Artificial Intelligence**\n",
        "\n",
        "-- It is a \"low-code\" infrastrcuture to build Medical Image Analysis Pipeleines\n",
        "\n",
        "MONAI consists of three frameworks:\n",
        "*   MONAI Label: seemlessly integrates into label generation workflow\n",
        "*   MONAI Core: enables clinicians and researchers to build AI models to work on **Medical Imaging** data\n",
        "*   MONAI Deploy: facilitates easy transition of Python programs into a deployable application\n",
        "\n",
        "\n",
        "This colab notebook will introduce you to the *MONAI Core*'s design and architecture. We will get hands-on examples with MONAI's quick introduction and preform a simple deeplearning task</p>\n",
        "\n"
      ],
      "metadata": {
        "id": "V4xCU9pkIMTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MONAI End to End workflow**\n",
        "\n",
        "MONAI supports deep learning in medical image analysis at multiple levels. This figure shows a typical example of an end-to-end workflow in a medical deep learning context:\n",
        "\n",
        "<center><img src=\"https://github.com/Project-MONAI/monai-bootcamp/blob/main/MONAICore/end_to_end.png?raw=1\" style=\"width: 1400px;\"/></center>"
      ],
      "metadata": {
        "id": "IYRp8UIU-Jfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Install & import*  **MONAI**\n",
        "\n",
        "MONAI is:   \n",
        "\n",
        "*   an open-source\n",
        "*   freely available\n",
        "*   collaborative framework\n",
        "*   **Low-code framework**\n",
        "\n",
        "built on **pyTorch** and **Python** for **accelarting research** & **clinical collaboration** in **Medical Image Analysis**\n",
        "\n"
      ],
      "metadata": {
        "id": "AGto4w5gJTAt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgFT37_b1iWE"
      },
      "outputs": [],
      "source": [
        "!pip install monai[all]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import monai\n",
        "from monai.config import print_config\n",
        "print_config()"
      ],
      "metadata": {
        "id": "b49OXCSx2NIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. **MONAI: Datatype**\n",
        "\n",
        "* Dataset: Combines data and its associated transform into a single entity\n",
        "  * Syntax: ``Dataset(data,transform=None)``\n",
        "\n",
        "    Where transform is an image or object manipulation that will be activated and acts on the data"
      ],
      "metadata": {
        "id": "m5UwsAHD3R3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from monai.data import Dataset\n",
        "from monai.transforms import ToTensor\n",
        "items = [{\"data\": 4},\n",
        "         {\"data\": 9},\n",
        "         {\"data\": 3},\n",
        "         {\"data\": 7},\n",
        "         {\"data\": 1},\n",
        "         {\"data\": 2},\n",
        "         {\"data\": 5}]\n",
        "\n",
        "print(type(items))\n",
        "\n",
        "transform_items = ToTensor()\n",
        "dataset = Dataset(items, transform=transform_items)\n",
        "print(type(dataset))\n",
        "print(dataset)\n",
        "print(f\"Length of dataset is {len(dataset)}\")\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "BiqF-aa6UDYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medical Image Analysis without MONAI**\n",
        "\n",
        "Let us explore why do we have a separate data type for MONAI?\n",
        "\n",
        "Traditionally, how we access them:\n",
        "\n",
        "1. import python imaging library (PIL)\n",
        "2. Import Numerical Python (numpy)\n",
        "3. Import pytorch\n",
        "4. Import matplotlib\n",
        "5. Convert the image into numpy array\n",
        "6. Show them as plot"
      ],
      "metadata": {
        "id": "TwkC4ZhyJ74s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a dummy image\n",
        "\n",
        "**Syntax**:\n",
        "```python\n",
        "monai.data.synthetic.create_test_image_2d(height, width, num_objs=12, num_seg_classes=1, channel_dim=3, random_state=None)\n",
        "```\n",
        "\n",
        "**Parameters**\n",
        "________\n",
        "**height** – height of the image.\n",
        "\n",
        "**width** – width of the image.\n",
        "\n",
        "**num_objs** – number of circles to generate. Defaults to 12.\n",
        "\n",
        "**rad_max** – maximum circle radius. Defaults to 30.\n",
        "\n",
        "**rad_min** – minimum circle radius. Defaults to 5.\n",
        "\n",
        "**noise_max** – if greater than 0 then noise will be added to the image taken from the uniform distribution on range [0,noise_max). Defaults to 0.\n",
        "\n",
        "**num_seg_classes** – number of classes for segmentations. Defaults to 5.\n",
        "\n",
        "**channel_dim** – if None, create an image without channel dimension, otherwise create an image with channel dimension as first dim or last dim. Defaults to None.\n",
        "\n",
        "**random_state** – the random generator to use. Defaults to np.random.\n",
        "\n"
      ],
      "metadata": {
        "id": "DfJzC18mUTVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from monai.data import create_test_image_2d\n",
        "\n",
        "image, seg = create_test_image_2d(height=128, width=128,num_objs=5,rad_max=10,rad_min=2,num_seg_classes=2)\n",
        "\n",
        "print(f\"Image shape: {image.shape}\")\n",
        "print(f\"Segmentation shape: {seg.shape}\")\n",
        "\n",
        "print(f\"Image min: {image.min()}, max: {image.max()}\")\n",
        "print(f\"Segmentation min: {seg.min()}, max: {seg.max()}\")"
      ],
      "metadata": {
        "id": "A1ERhLGrUS3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization\n",
        "\n",
        "- Matplotlib has a number of built-in colormaps\n",
        "- An intuitive color scheme for the parameter you are plotting\n",
        "- More details on ``matplotlib.colormaps`` is available here: https://matplotlib.org/stable/users/explain/colors/colormaps.html\n"
      ],
      "metadata": {
        "id": "MtlnEgyRhtvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://matplotlib.org/stable/_images/sphx_glr_colormaps_014.png\">"
      ],
      "metadata": {
        "id": "CwnPnYBDeUG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(\"visualize\", (12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"image\")\n",
        "plt.imshow(image, cmap=\"gray\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"segmentation\")\n",
        "plt.imshow(seg,cmap=\"gnuplot\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t8f4s2M6ga09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MONAI Dataset"
      ],
      "metadata": {
        "id": "NDJoMJB837V_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from monai.data import Dataset\n",
        "\n",
        "data = [\n",
        "    {\"image\": image, \"seg\": seg}\n",
        "]\n",
        "\n",
        "# Define a dataset using the data list\n",
        "dataset = Dataset(data=data)\n",
        "\n",
        "\n",
        "print(f\"Dataset length: {len(dataset)}\")\n",
        "\n",
        "# Access a data item by index\n",
        "item = dataset[0]\n",
        "print(f\"Keys in item: {item.keys()}\")\n",
        "\n",
        "print(f\"Image shape: {item['image'].shape}\")\n",
        "print(f\"Segmentation shape: {item['seg'].shape}\")\n"
      ],
      "metadata": {
        "id": "1Rr6lrVDjEs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. **MONAI: Transforms**\n",
        "\n",
        "A lot of geometric and image inherent transforms are available to augment the data available and they are easily clubbed with ``Compose`` and image loading\n",
        "\n",
        "Today we will integrate the following image transforms:\n",
        "* ``ScaleIntensityd``\n",
        "* ``Resized``\n",
        "* ``RandRotated``\n",
        "* ``RandFlipd``\n",
        "* ``RandAdjustContrastd``\n",
        "* ``RandAxisFlipd``\n",
        "* ``RandZoomd``\n",
        "* ``RandRotate90d``\n",
        "* ``ToNumpyd``\n",
        "\n",
        "Integrating these functions into the transform methodically will help us get accustomed to the MONAI's transform syntax\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XcXhdgvtMbpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import monai\n",
        "from monai.transforms import Compose, EnsureChannelFirstd, ToTensord\n",
        "from monai.transforms import ScaleIntensityd, Resized, RandRotated, RandFlipd, RandAdjustContrastd, RandAxisFlipd, RandZoomd, RandRotate90d, ToNumpyd\n",
        "\n",
        "# Define a transform to convert image and segmentation into tensors,\n",
        "# ensure channel first and scale intensity\n",
        "transform = Compose([\n",
        "    EnsureChannelFirstd(keys=[\"image\", \"seg\"],channel_dim=\"no_channel\"),\n",
        "    ScaleIntensityd(keys=[\"image\", \"seg\"]),\n",
        "    Resized(keys=[\"image\"], spatial_size=(100,100)),\n",
        "    RandRotated(keys=[\"image\", \"seg\"], range_x=0.3, prob=0.5, mode=['bilinear', 'nearest']),\n",
        "    RandFlipd(keys=[\"image\", \"seg\"], prob=0.5, spatial_axis=0),\n",
        "    RandAdjustContrastd(keys=[\"image\"], prob=0.5, gamma=(0.5, 2.0)),\n",
        "    RandAxisFlipd(keys=[\"image\", \"seg\"], prob=0.5),\n",
        "    RandZoomd(keys=[\"image\", \"seg\"], prob=0.5, min_zoom=0.8, max_zoom=1.2, mode=['area', 'nearest']),\n",
        "    RandRotate90d(keys=[\"image\", \"seg\"], prob=0.5, spatial_axes=(0, 1)),\n",
        "    ToTensord(keys=[\"image\", \"seg\"])\n",
        "])\n",
        "# Create a monai dataset with the transform\n",
        "dataset_transformed = monai.data.Dataset(data=data, transform=transform)\n",
        "\n",
        "\n",
        "# Access a data item by index\n",
        "item = dataset_transformed[0] # Assuming 'dataset_transformed' is the correct dataset object.  The original code used 'dataset'\n",
        "print(f\"Keys in item: {item.keys()}\")\n",
        "\n",
        "print(f\"Image shape: {item['image'].shape}\")\n",
        "print(f\"Segmentation shape: {item['seg'].shape}\")\n",
        "\n",
        "print(f\"Image min: {item['image'].min()}, max: {item['image'].max()}\")\n",
        "print(f\"Segmentation min: {item['seg'].min()}, max: {item['seg'].max()}\")\n",
        "\n",
        "# Access a data item by index\n",
        "item = dataset[0]\n",
        "print(f\"Keys in item: {item.keys()}\")\n",
        "\n",
        "print(f\"Image shape: {item['image'].shape}\")\n",
        "print(f\"Segmentation shape: {item['seg'].shape}\")\n",
        "\n",
        "print(f\"Image min: {item['image'].min()}, max: {item['image'].max()}\")\n",
        "print(f\"Segmentation min: {item['seg'].min()}, max: {item['seg'].max()}\")"
      ],
      "metadata": {
        "id": "g9bEs-eTjJoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Assuming 'image' and 'seg' are from dataset_transformed\n",
        "# Access the transformed data\n",
        "item_transformed = dataset_transformed[0]\n",
        "image_transformed = item_transformed['image'].numpy()  # Access transformed image\n",
        "seg_transformed = item_transformed['seg'].numpy()    # Access transformed segmentation\n",
        "\n",
        "item = dataset[0]\n",
        "image = item['image']\n",
        "seg = item['seg']\n",
        "\n",
        "\n",
        "# Remove channel dimension if present for transformed images\n",
        "if len(image_transformed.shape) == 3 and image_transformed.shape[0] == 1:\n",
        "    image_transformed = image_transformed.squeeze(0)\n",
        "if len(seg_transformed.shape) == 3 and seg_transformed.shape[0] == 1:\n",
        "    seg_transformed = seg_transformed.squeeze(0)\n",
        "\n",
        "# Remove channel dimension if present for original images\n",
        "if len(image.shape) == 3 and image.shape[0] == 1:\n",
        "    image = image.squeeze(0)\n",
        "if len(seg.shape) == 3 and seg.shape[0] == 1:\n",
        "    seg = seg.squeeze(0)\n",
        "\n",
        "# Create a 2x2 subplot to display all four images\n",
        "plt.figure(\"visualize\", (12, 12))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.title(\"Original Image\")\n",
        "plt.imshow(image, cmap=\"gray\")\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.title(\"Original Segmentation\")\n",
        "plt.imshow(seg, cmap=\"gnuplot\")\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.title(\"Transformed Image\")\n",
        "plt.imshow(image_transformed, cmap=\"gray\")\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.title(\"Transformed Segmentation\")\n",
        "plt.imshow(seg_transformed, cmap=\"gnuplot\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Esr7vGE09CgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. **MONAI datasets**\n",
        "\n",
        "From MONAI Applications: ``class monai.apps``\n",
        "- MEDNIST\n",
        "- Medical Decathlon\n",
        "- TCIA\n",
        "- Others: MEDMNIST\n",
        "- Others: PhysioNet"
      ],
      "metadata": {
        "id": "93HGiArEjQ18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "dir_path = os.getcwd()\n",
        "print(dir_path)"
      ],
      "metadata": {
        "id": "0ONpkA7tlWas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MEDNIST Dataset\n",
        "\n",
        "The MedNIST dataset was gathered from several sets from [TCIA](https://wiki.cancerimagingarchive.net/display/Public/Data+Usage+Policies+and+Restrictions),\n",
        "[the RSNA Bone Age Challenge](http://rsnachallenges.cloudapp.net/competitions/4),\n",
        "and [the NIH Chest X-ray dataset](https://cloud.google.com/healthcare/docs/resources/public-datasets/nih-chest).\n",
        "\n",
        "The dataset is kindly made available by [Dr. Bradley J. Erickson M.D., Ph.D.](https://www.mayo.edu/research/labs/radiology-informatics/overview) (Department of Radiology, Mayo Clinic)\n",
        "under the Creative Commons [CC BY-SA 4.0 license](https://creativecommons.org/licenses/by-sa/4.0/). If you use the MedNIST dataset, please acknowledge the source.\n",
        "\n",
        "Syntax: ``MedNISTDataset(root_dir, section, transform=(), download=False, seed=0, val_frac=0.1, test_frac=0.1, cache_num=9223372036854775807, cache_rate=1.0, num_workers=1, progress=True, copy_cache=True, as_contiguous=True, runtime_cache=False)``\n",
        "\n",
        "**Parameters**:\n",
        "- **root_dir** – target directory to download and load MedNIST dataset.\n",
        "- **section** – expected data section, can be: training, validation or test.\n",
        "- **download** – whether to download and extract the MedNIST from resource link, default is False. if expected file already exists, skip downloading even set it to True. user can manually copy MedNIST.tar.gz file or MedNIST folder to root directory.\n",
        "- **seed** – random seed to randomly split training, validation and test datasets, default is 0.\n",
        "- **val_frac** – percentage of validation fraction in the whole dataset, default is 0.1.\n",
        "- **test_frac** – percentage of test fraction in the whole dataset, default is 0.1."
      ],
      "metadata": {
        "id": "Rd6hzoPinFYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from monai.apps import MedNISTDataset\n",
        "train_data = MedNISTDataset(root_dir=dir_path, section=\"training\",download=False, seed=24, val_frac=0.1, test_frac=0.7)\n",
        "val_data = MedNISTDataset(root_dir=dir_path, section=\"validation\",download=False, seed=24, val_frac=0.1, test_frac=0.7)\n",
        "test_data = MedNISTDataset(root_dir=dir_path, section=\"test\",download=False, seed=24, val_frac=0.1, test_frac=0.7)\n"
      ],
      "metadata": {
        "id": "ZhIIj5vPk5Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length of training dataset: {len(train_data)}\")\n",
        "print(f\"Length of validation dataset: {len(val_data)}\")\n",
        "print(f\"Length of test dataset: {len(test_data)}\")\n",
        "print(f\"Type of train data: {type(train_data)}\")\n",
        "# print(dir(train_data))\n",
        "for data in val_data:\n",
        "    image = data['image']\n",
        "    label = data['label']\n",
        "    # Print or inspect the 'image' and 'label'\n",
        "    print(f\"Image shape: {image.shape}, Label: {label}\")"
      ],
      "metadata": {
        "id": "7ATpeCCNmI8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data exploration\n",
        "\n",
        "First of all, check the dataset files and show some statistics.  \n",
        "There are 6 folders in the dataset: Hand, AbdomenCT, CXR, ChestCT, BreastMRI, HeadCT, which should be used as the labels to train our classification model."
      ],
      "metadata": {
        "id": "9U-SEgiJpce2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "mednist_folder = os.path.join(dir_path, 'MedNIST')\n",
        "\n",
        "if os.path.exists(mednist_folder):\n",
        "  for subfolder in os.listdir(mednist_folder):\n",
        "    subfolder_path = os.path.join(mednist_folder, subfolder)\n",
        "    if os.path.isdir(subfolder_path):\n",
        "      print(f\"Subfolder: {subfolder}\")\n",
        "else:\n",
        "  print(\"MedNIST folder not found.\")\n"
      ],
      "metadata": {
        "id": "J6CNrNItpfYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "mednist_folder = os.path.join(dir_path, 'MedNIST')\n",
        "\n",
        "class_names = sorted(x for x in os.listdir(mednist_folder) if os.path.isdir(os.path.join(mednist_folder, x)))\n",
        "num_class = len(class_names)\n",
        "image_files = [\n",
        "    [os.path.join(mednist_folder, class_names[i], x) for x in os.listdir(os.path.join(mednist_folder, class_names[i]))]\n",
        "    for i in range(num_class)\n",
        "]\n",
        "num_each = [len(image_files[i]) for i in range(num_class)]\n",
        "image_files_list = []\n",
        "image_class = []\n",
        "for i in range(num_class):\n",
        "    image_files_list.extend(image_files[i])\n",
        "    image_class.extend([i] * num_each[i])\n",
        "num_total = len(image_class)\n",
        "image_width, image_height = PIL.Image.open(image_files_list[0]).size\n",
        "\n",
        "print(f\"Total image count: {num_total}\")\n",
        "print(f\"Image dimensions: {image_width} x {image_height}\")\n",
        "print(f\"Label names: {class_names}\")\n",
        "print(f\"Label counts: {num_each}\")"
      ],
      "metadata": {
        "id": "4kVpod-cqROa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplots(3, 3, figsize=(8, 8))\n",
        "for i, k in enumerate(np.random.randint(num_total, size=9)):\n",
        "    im = PIL.Image.open(image_files_list[k])\n",
        "    arr = np.array(im)\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    plt.xlabel(class_names[image_class[k]])\n",
        "    plt.imshow(arr, cmap=\"gray\", vmin=0, vmax=255)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0WroCQhVrXPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decathlon datasets\n",
        "\n",
        "- The Dataset command to automatically download the data of Medical Segmentation Decathlon challenge (http://medicaldecathlon.com/) and generate items for training, validation or test.\n",
        "\n",
        "- It will also load these properties from the JSON config file of dataset.\n",
        "\n",
        "- Syntax:\n",
        "```python\n",
        "DecathlonDataset(root_dir, task, section, download=False, seed=0, val_frac=0.2, progress=True)\n",
        "```\n",
        "Parameters:\n",
        "- **root_dir** – local directory for caching and loading the MSD datasets.\n",
        "\n",
        "- **task** – Task to download and execute: one item of the list\n",
        "    - “Task01_BrainTumour”\n",
        "    - “Task02_Heart”\n",
        "    - “Task03_Liver”\n",
        "    - “Task04_Hippocampus”\n",
        "    - “Task05_Prostate”\n",
        "    - “Task06_Lung”\n",
        "    - “Task07_Pancreas”\n",
        "    - “Task08_HepaticVessel”\n",
        "    - “Task09_Spleen”\n",
        "    - “Task10_Colon”\n",
        "\n",
        "- **section** – expected data section: training or validation.\n",
        "\n",
        "- **download** – whether to download and extract the Decathlon from resource link, default is False. if expected file already exists, skip downloading even set it to True. user can manually copy tar file or dataset folder to the root directory.\n",
        "\n",
        "- **val_frac** – percentage of validation fraction in the whole dataset, default is 0.2.\n",
        "\n",
        "- **seed** – random seed to randomly shuffle the datalist before splitting into training and validation, default is 0.\n",
        "  - **Note**: Set same seed for training and validation sections.\n",
        "\n",
        "- **progress** – whether to display a progress bar when downloading dataset and computing the transform cache content.\n",
        "\n"
      ],
      "metadata": {
        "id": "1I8Y0G68sf_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from monai.apps import DecathlonDataset\n",
        "\n",
        "# Specify the task number you want to access (e.g., Task04_Hippocampus)\n",
        "task_num = \"Task04_Hippocampus\"\n",
        "\n",
        "# Create a DecathlonDataset instance for the specified task\n",
        "train_decathlondataset = DecathlonDataset(root_dir=dir_path, task=task_num, section=\"training\", download=True,val_frac=0.1)\n",
        "validation_decathlondataset = DecathlonDataset(root_dir=dir_path, task=task_num, section=\"validation\", download=False,val_frac=0.1)"
      ],
      "metadata": {
        "id": "F9Ofq1EL8q9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TCIA Dataset\n",
        "\n",
        "- The Dataset to automatically download the data from a public The Cancer Imaging Archive (TCIA) dataset and generate items for training, validation or test. [https://www.cancerimagingarchive.net/](https://www.cancerimagingarchive.net/)\n",
        "- Syntax:\n",
        "```Python\n",
        "class monai.apps.TciaDataset(root_dir, collection, section, transform=(), download=False)\n",
        "```\n",
        "\n",
        "- **Massive Public Database**: TCIA provides a huge collection of de-identified medical images (like CT scans, MRIs, and histopathology slides) across a wide range of cancer types. This allows researchers to access diverse data for analysis, development of image-based diagnostic tools, and discovery of new disease insights.\n",
        "\n",
        "- **Open and Free**: All the data in TCIA is freely available to the public. This open access promotes collaboration, accelerates research, and encourages the development of innovative cancer imaging applications.\n",
        "\n",
        "- **Standardized Format**: TCIA uses the DICOM (Digital Imaging and Communications in Medicine) standard for storing and distributing images. This ensures compatibility and makes it easier for researchers to use the data with various image processing and analysis tools."
      ],
      "metadata": {
        "id": "Bg-wBjZ2s50Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydicom\n",
        "import pydicom\n",
        "from monai.apps import TciaDataset\n",
        "\n",
        "# Specify the collection you want to access (e.g., \"Lung Phantom\")\n",
        "collection = \"Lung Phantom\"\n",
        "\n",
        "# Create a TciaDataset instance for the specified collection\n",
        "tcia_dataset = TciaDataset(root_dir=dir_path, collection=collection, section=\"training\", download=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "hfHabrVBtAAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. **MONAI: Simple Deeplearning Task**"
      ],
      "metadata": {
        "id": "WYrt8P7_kSqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. **MONAI: DataLoader**\n",
        "\n"
      ],
      "metadata": {
        "id": "wdFBK3O6OjkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from monai.data import DataLoader\n",
        "\n",
        "train_transforms = Compose(\n",
        "    [\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"],channel_dim=\"no_channel\"), # Apply EnsureChannelFirstd to the \"image\" key\n",
        "        Resized(keys=[\"image\"], spatial_size=(32,32)), # Apply Resized to the \"image\" key\n",
        "        ToTensord(keys=[\"image\", \"label\"]), # Convert the \"image\" and \"label\" keys to tensors\n",
        "    ]\n",
        ")\n",
        "train_dataset = Dataset(data=train_data, transform=train_transforms)\n",
        "val_dataset = Dataset(data=val_data, transform=train_transforms)\n",
        "test_dataset = Dataset(data=test_data, transform=train_transforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "print(f\"Length of train_loader: {len(train_loader)}\")\n",
        "print(f\"Length of val_loader: {len(val_loader)}\")\n",
        "print(f\"Length of test_loader: {len(test_loader)}\")\n",
        "print(f\"The shape of train_loader: {train_loader.dataset[0]['image'].shape}\")\n",
        "print(f\"The sahe of train_loader: {train_loader.dataset[0]['label'].shape}\")"
      ],
      "metadata": {
        "id": "7auDYr_4PG0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. **MONAI: Model**\n",
        "\n",
        "- MONAI provides specialized neural network architectures and pre-trained models for medical imaging tasks, optimized for handling 3D volumetric data and different modalities (like MRI and CT).\n",
        "\n",
        "- These models are designed to improve the accuracy and efficiency of tasks such as image segmentation, classification, and registration in medical applications.\n",
        "\n",
        "- More details: https://docs.monai.io/en/stable/networks.html\n"
      ],
      "metadata": {
        "id": "K6StQ-fOS6yW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from monai.networks.nets import DenseNet121\n",
        "model = DenseNet121(spatial_dims=2, in_channels=1, out_channels=num_class)"
      ],
      "metadata": {
        "id": "oTFink95S9FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. **MONAI: Loss function**\n",
        "\n",
        "- MONAI offers specialized loss functions tailored for medical imaging, addressing challenges like class imbalance and volumetric data.\n",
        "- These functions, including Dice Loss and Focal Loss variants, optimize model training for segmentation, classification, and other medical image analysis tasks.\n",
        "\n",
        "- More details: https://docs.monai.io/en/stable/losses.html"
      ],
      "metadata": {
        "id": "FkcKOWW4Tyq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from monai.losses import DiceCELoss\n",
        "\n",
        "loss_function = DiceCELoss(to_onehot_y=True, lambda_dice=0,lambda_ce=1.0,softmax=False)"
      ],
      "metadata": {
        "id": "6-MbvU1mUInz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. **MONAI: Metrics**\n",
        "\n",
        "- They allow the use of tensors in the parameter calculations\n",
        "- More details: https://docs.monai.io/en/stable/metrics.html\n"
      ],
      "metadata": {
        "id": "xAzg_gsCVJps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from monai.metrics import ROCAUCMetric\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "auc_metric = ROCAUCMetric()\n"
      ],
      "metadata": {
        "id": "2IVapZYiVO6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer"
      ],
      "metadata": {
        "id": "V3QPQyb3VbdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-5)"
      ],
      "metadata": {
        "id": "izYMkHI6VdwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "GViXSVa2VUpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from monai.transforms import AsDiscrete, Activations\n",
        "y_pred_trans = Activations(softmax=True)                          #added y_pred_trans for softmax\n",
        "y_trans = AsDiscrete(to_onehot=num_class)                                 #added y_trans for one_hot\n",
        "from monai.data import decollate_batch\n",
        "\n",
        "max_epochs = 1\n",
        "val_interval = 1\n",
        "\n",
        "from tqdm import tqdm\n",
        "best_metric = -1\n",
        "best_metric_epoch = -1\n",
        "epoch_loss_values = []\n",
        "metric_values = []\n",
        "\n",
        "\n",
        "for epoch in range(max_epochs):                                                                   #Iteration of for loop through multiple epochs\n",
        "    print(\"-\" * 10)\n",
        "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    step = 0\n",
        "    for batch_data in tqdm(train_loader):                                                                              #Iteration of all the data in train loader\n",
        "        step += 1\n",
        "        inputs, labels = batch_data[\"image\"], batch_data[\"label\"]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)                                                                                 #Predicting the outputs from the model\n",
        "        loss = loss_function(outputs, labels)                                                                   #Computing the loss for each batch\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        # print(f\"{step}/{len(train_dataset) // train_loader.batch_size}, \" f\"train_loss: {loss.item():.4f}\")\n",
        "        epoch_len = len(train_dataset) // train_loader.batch_size\n",
        "    epoch_loss /= step\n",
        "    epoch_loss_values.append(epoch_loss)\n",
        "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")                                                  #Printing the computed loss after training\n",
        "\n",
        "    if (epoch + 1) % val_interval == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            y_pred = torch.tensor([], dtype=torch.float32)\n",
        "            y = torch.tensor([], dtype=torch.long)\n",
        "            for val_data in val_loader:                                                                         #Iteration of all the data in val loader\n",
        "                val_images, val_labels = val_data[\"image\"], val_data[\"label\"]\n",
        "                y_pred = torch.cat([y_pred, model(val_images)], dim=0)\n",
        "                y = torch.cat([y, val_labels], dim=0)\n",
        "            y_onehot = [y_trans(i) for i in decollate_batch(y, detach=False)]\n",
        "            # print(y_onehot)\n",
        "            y_pred_act = [y_pred_trans(i) for i in decollate_batch(y_pred)]\n",
        "            # print(y_pred_act.shape)\n",
        "            # y_onehot = torch.stack(y_onehot)\n",
        "            # y_pred_act = torch.stack(y_pred_act)\n",
        "            auc_metric(y_pred_act, y_onehot)                                                                    #Computing the AUROC metric\n",
        "            result = auc_metric.aggregate()\n",
        "            auc_metric.reset()\n",
        "            del y_pred_act, y_onehot\n",
        "            metric_values.append(result)\n",
        "            y_pred_class = torch.argmax(y_pred, dim=1)                          # Convert logits to class labels\n",
        "            acc_metric = accuracy_score(y.cpu().numpy(),y_pred_class.cpu().numpy())                             # Computing the accuracy\n",
        "            if result > best_metric:\n",
        "                best_metric = result\n",
        "                best_metric_epoch = epoch + 1\n",
        "                torch.save(model.state_dict(), \"best_metric_model.pth\")\n",
        "                print(\"saved new best metric model\")\n",
        "            print(\n",
        "                f\"current epoch: {epoch + 1} current AUC: {result:.4f}\"\n",
        "                f\" current accuracy: {acc_metric:.4f}\"\n",
        "                f\" best AUC: {best_metric:.4f}\"\n",
        "                f\" at epoch: {best_metric_epoch}\"\n",
        "            )\n",
        "\n",
        "print(f\"train completed, best_metric: {best_metric:.4f} \" f\"at epoch: {best_metric_epoch}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "XF-iHlkEVYCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "CiXNnW_wZ1UZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(\"/content/best_metric_model.pth\"))\n",
        "\n",
        "#Setting the model to evaluation state\n",
        "model.eval()\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_pred = torch.tensor([], dtype=torch.float32)\n",
        "    y = torch.tensor([], dtype=torch.long)\n",
        "    for val_data in val_loader:\n",
        "        val_images, val_labels = val_data[\"image\"], val_data[\"label\"]\n",
        "        y_pred = torch.cat([y_pred, model(val_images)], dim=0)\n",
        "        y = torch.cat([y, val_labels], dim=0)\n",
        "\n",
        "print(f\"The shape of y_pred is: {y_pred.shape}\")\n",
        "y_pred_class = torch.argmax(y_pred,dim=1)\n",
        "print(f\"The shape of y_pred_class is: {y_pred_class.shape}\")\n",
        "print(f\"The shape of y_pred_class is: {y_pred_class.unsqueeze(-1).shape}\")\n",
        "accuracy = accuracy_score(y_pred_class.cpu().numpy(),y.squeeze(-1).cpu().numpy())\n",
        "print(f\"Accuracy on validation set: {accuracy}\")\n",
        "y_onehot = [y_trans(i) for i in decollate_batch(y, detach=False)]\n",
        "y_pred_onehot = [y_trans(i) for i in decollate_batch(y_pred_class.unsqueeze(-1), detach=False)]\n",
        "y_onehot = torch.stack(y_onehot)\n",
        "y_pred_onehot = torch.stack(y_pred_onehot)\n",
        "print(f\"The shape of y_pred_onehot is: {y_pred_onehot.shape}\")\n",
        "print(f\"The shape of y_onehot is: {y_onehot.shape}\")\n",
        "# print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "conf_matix_sklearn = confusion_matrix(y,y_pred_class)\n",
        "# print(f\"Confusion Matrix:\\n{conf_matix_sklearn}\")\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y.cpu().numpy(), y_pred_class.cpu().numpy(), target_names=class_names))\n"
      ],
      "metadata": {
        "id": "kgYiL5tDaDDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming conf_matix_sklearn is your confusion matrix from sklearn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matix_sklearn, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oQm5PaRJaaKP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}